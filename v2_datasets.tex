\chapter{Installations and datasets}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{pics/fmikumpula}
\figcaption{FMI Kumpula solar power installation string.}
\label{fig_fmikumpula_panels}
\end{figure}


\noindent The two datasets used in this thesis were provided by FMI. They contain power generation measurements from installations in Kuopio and Helsinki, with the physical parameters listed in table \ref{table_fmi_helsinki_kuopio_parameters}. Due to the high elevation of the installations, shading is unlikely to be a major factor in either of the datasets. The same data was previously used in Herman Böök's \textit{Photovoltaic output modeling}\cite{hbook1} and thus installation parameters and datasets have previously been verified.

The data in the two datasets follows a similar structure as shown in table \ref{table_fmi_kumpula_csv}. This snapshot from the Helsinki dataset shows that the temporal resolution is one measurement per minute and that there are multiple power values for each minute. The fields String 1 and String 2 represent power from two identical sets or strings of solar PV panels installed at the same location and their sum should be near equal to the inverter input. One of these strings is shown in figure \ref{fig_fmikumpula_panels}. Datasets from other sources may differ from the FMI datasets in several ways. Power measurements may be taken at different intervals, once per 1, 5, 15, or 60 minutes, and they are unlikely to contain more than one power value. Due to these reasons, the algorithms in this thesis are designed to operate on datasets with one measurement per minute, and they use only the inverter output value as that is the most likely power value included in solar PV datasets.


\begin{table}[h]

\centering

\begin{tabular}{r|cccc} \hline\hline

Timestamp[UTC] & Inverter out & Inverter in & String 1 & String 2\\ \hline
$2015-08-26$ $03:34$ & $NaN$ & $NaN$ & $0.5$ & $NaN$\\
$2015-08-26$ $03:36$ & $11.1$ & $7.5$ & $2.6$ & $4.9$\\
$2015-08-26$ $03:37$ & $25.4$ & $26.1$ & $9.8$ & $16.3$\\
$2015-08-26$ $03:38$ & $30.7$& $NaN$ & $NaN$ & $0.4$\\
$2015-08-26$ $03:39$ & $46.4$& $44.8$ & $20$ & $24.8$\\
$2015-08-26$ $03:40$ & $3.3$ & $NaN$ & $NaN$ & $0.4$\\
$2015-08-26$ $03:41$ & $29.3$ &  $18$ & $9.1$ & $8.9$\\
$2015-08-26$ $03:42$ & $33.1$& $27.4$ & $10.6$ & $16.9$\\

\vdots & \vdots & \vdots & \vdots & \vdots\\
$2015-08-26$ $12:42$ & $12374.8$ & $14619.1$ & $7152$ & $7467.1$\\
$2015-08-26$ $12:43$ & $15442.2$ & $15482.1 $& $7708.9$ & $7773.2$\\
$2015-08-26$ $12:44$ & $14085.8$ & $12898.7$ & $6387$ & $6511.8$ \\
\vdots & \vdots & \vdots & \vdots & \vdots\\

\hline\hline
\end{tabular}

\tabcaption{A section from FMI's Kumpula solar site PV production data, only the timestamp and inverter output values are used by the algorithms in this thesis. All power measurements are in watts.}
\label{table_fmi_kumpula_csv}
\end{table}





\begin{table}[H]
\centering
\begin{tabular}{r|cc} \hline\hline

 & Helsinki & Kuopio\\ \hline
 Latitude & $60.204^\circ$ & $62.892^\circ$ \\
 Longitude & $24.961^\circ$  &  $27.634^\circ$\\
 Nominal capacity &21 kW & 20.28 kW \\
 Panel tilt & $15^\circ$ & $15^\circ$ \\
 Panel azimuth & $135^\circ$ & $217^\circ$ \\
 Elevation & 17m & 10m\\
\hline\hline
\end{tabular}
\tabcaption{Parameters for the FMI's Kumpula(Helsinki) and Kuopio PV installations as listed in Böök 2020 \cite{hbook1}.}
\label{table_fmi_helsinki_kuopio_parameters}
\end{table}


\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{pics/irradiancetypes}
\figcaption{Simplified installation diagram for a PV system in ideal conditions.}
\label{fig_simplediagram}
\end{figure}

\noindent
Figure \ref{fig_simplediagram} shows a two dimensional simplification of a solar PV installation. The before unmentioned abbreviation AOI stands for angle of incidence and it is defined as the angle between direct irradiance and solar panel normal. AOI can be calculated with computer programs and software libraries when geolocation, panel tilt and azimuth and the time are known. The azimuth angle is not marked on the figure and azimuth represents the second panel normal component. Azimuth for geographic north is 0 degrees and azimuth is measured in clockwise degrees from north. The angles given for Helsinki($135^\circ$) and Kuopio($217^\circ$) installations tell us that the panels are facing southeast and southwest respectively. 



\section{Visualizing the data}
The figure \ref{fig_oneyear_pointcloud} contains a 3D point cloud generated by plotting one year of data from FMI Helsinki dataset and it shows that there are visible structures in the data. The clerest structure in the 3D -plot is the pattern formed by the first and last non-zero power minutes and this is later used for geolocation estimation. The second pattern is the dome-like shape of the point cloud. This shape can be examined by taking one day slices from the dataset and plotting them individually as shown in \ref{fig_cloudfree_vs_cloudy}. These slices are used for panel installation angle estimation.%The process of installation angle estimation is complicated by weather phenomena and thus a method for filtering out noisy days is needed.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{pics/oneyear2}
\figcaption{One year of data from FMI Kumpula installation as a 3D point cloud.}
\label{fig_oneyear_pointcloud}
\end{figure}

\newpage
\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{pics/cloudfree_vs_cloudy}
\figcaption{Two days from FMI Kumpula dataset with different charasteristics.}
\label{fig_cloudfree_vs_cloudy}
\end{figure}

The presumably cloud free day 2017-123 in \ref{fig_cloudfree_vs_cloudy} has some notable charasteristics in addition to the geolocation derived first and last minutes. The shape resembles a skewed normal distribution and the knee section near 950 minutes is likely caused by the transition from direct sunlight to atmosphere scattered sunlight. This happens when the angle of incidence reaches 90 degrees.

Another measurable trait is the peak power generation minute. Geometric intuition would suggest that this should align with the moment in time with the lowest angle of incidence but this relationship could be fairly complex due to temperature induced efficiency variation and atmopshere induced losses which are higher when the Sun elevation is low.



%Note that this transition appears to be smooth and this may be a result of high reflective losses at high AOI. Similarly, intuition would suggest that the peak power minute occurs when the angle of incidence is at its minimum. Measurable traits such as these could have uses for parameter estimation.
%be used for parameter estimation, but the relationships between figure traits and system parameters can be complicated.


\newpage
\section{Data pre-processing}
The data pre-processing required by the algorithms in this thesis can be split into two gategories, classifying preprocessing and reparing preprocessing. Classifying preprocessing is used to determine if a certain section of data is useful of analysis or not, the primary example here is the cloud free day detection algorithm which is discussed more throroughly in the next chapters. The second type of preprocessing, reparing preprocessing refers to the use of algorithms which fill gaps measurement data or otherwise attempt to repair data which is unusable as is, but which could be used after repairing.

The data preprocessing algorithms used in this thesis load the data from csv files and examine whether individual days in the dataset meet set qualification requirements. These are the minimum and maximum measurement count, whether first and last measurements are taken too close to minute 0 or 1440 and the the percentage of measurements included between the first and last measurement. Figure \ref{fig_accepted_days} contains a comparison on which days in the datasets met the requirements.


\begin{figure}[h]
	
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{pics/helsinki_accepted_days}
         \caption{Days in Helsinki dataset which met data quality thresholds.}
         \label{fig_helsinki_accepted}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{pics/kuopio_accepted_days}
         \caption{Days in Kuopio dataset which met data quality thresholds.}
         
         \label{fig_kuopio_accepted}
     \end{subfigure}
     \hfill
     \caption{Requirements were measurement count between 400 and 1200, first minute is 5th or later, last minute is 1435 or earlier. More than 95\% of measurements between first and last minute must be included.}
     \label{fig_accepted_days}
     
\end{figure}


After acceptable days are chosen with the classification algorithm, the next step is data repairing. Missing measurements and Nan values can be linearly interpolated, meaning that if a power measurement or a set of measurements is missing between two datapoints, the missing datapoints are estimated to describe a linear transition breaching the gap between the known datapoints. When noise level is low, linearly approximating the missing values is unlikely to result in signficant errors. After this is done, the resulting data is ready for analysis.



\section{Clear day detection algorithm}
\label{clearskyalgo_chapter}
While the previous preprocessing steps have filtered and repaired days according to measurement counts and data gaps, these algorithms did not filter days based on the amplitude noise present in power measurements. This power noise is often caused by clouds and the resulting generation curves can not be reliably used for curve fitting. The following algorithm steps can be used for automating the process of selecting days with low amounts of high-frequency changes


% If the interference in measurements caused by clouds or other sources is significant, the value of a day for model fitting is reduced. An example of strong interference can be seen in figure \ref{fig_cloudfree_vs_cloudy}. Detecting the presence of such interference with an algorithm would help with automating the process of model fitting as that would eliminate the need to manually select good days from datasets. The following steps describe the process used for cloud free day detection in this thesis.



%\noindent \textbf{Algorithm step by step:}

\begin{enumerate}
  \item Dataset is split into days based on utc timestamps.
  
  \item A copy of the power measurements for each individual day is taken and fed to a low pass filter algorithm.
  
  
  \item A delta value is calculated based on the difference between original power measurements and low pass filtered measurements.

  
  \item Days are discarded if their delta values fail to meet a set threshold. 

  
  
   %If the average difference from step 5 is on average higher than a given treshold value, reject the day.
\end{enumerate}



\noindent The mathematically non-trivial parts here are threshold selection, difference measurement and low-pass filtering. Low-pass filtering is a term borrowed from the field of signal processing, and it refers to any algorithm that removes frequencies higher than a given limit from a signal, allowing lower frequencies to pass. 



Here the filtering is done with discrete Fourier transformations DFT and inverse discrete Fourier transformations IDFT. When a list of numbers is used as the input of DFT, the output is a list of ordered complex numbers, each of which represents a sine wave of a certain frequency, phase and amplitude. The sum of these wave equations forms a continuous approximation of the input values and by sampling the continuous representation, the continuous trigonometric approximation can be transformed back into discrete values. However if the complex numbers are adjusted before the IDFT operation, frequencies can be selectively modified. This means that DFT and IDFT can be used for frequency specific modification of numerical lists, low-pass filtering being one of the possibilities. In this case the low-pass filtering was accomplished by zeroing out complex numbers which do not correspond to the 6 longest frequencies, the resulting smoothening can be seen in figure \ref{fig_cloudfree_algo}.


While this process is somewhat complicated, Fourier transformations are not the only tool for creating low pass filters. Similar results can also be achieved by locally averaging each power value to be the average of nearest $k$ values. Discrete Fourier transformation based methods do however have an advantage in their universality. If the 6 or 7 or $n$ longest frequencies can be determined to be a good low pass filter, then these same frequencies should result in similar outputs no matter the temporal resolution of the power measurement data. Where as a method based on local averages would require a different window size depending on measurement intervals.

The second component is not as complicated as the low pass filtering operation. Measuring the delta between a filtered and unfiltered set of measurements can be done by computing the discrete curve length or as was done here, measuring the absolute average deviation between filtered and unfiltered power measurement. This is shown with the following equations \ref{eq2-1}-\ref{eq2-5}.


\begin{align}
Power &= [p_0, p_1, p_2, \dots , p_n]   \label{eq2-1}\\ 
Power_{filtered} &= [f_0, f_1, f_2, \dots , f_n] \\
Power_{delta} &= [|p_0 - f_0|, |p_1-f_1|, |p_2-f_2|, \dots , |p_n-f_n|] \\
delta_{avg} &= avg(Power_{delta}) \\
delta_{norm} &= delta_{avg}/ max(Power) \label{eq2-5}
\end{align}


The last component is threshold selection. The value $delta_{avg}$ describes the average wattage difference between measured and low pass filtered measured power values. By definition, this delta value is dependent on noise and installation size, limiting its usability. A noise only -delta value can be calculated by normalizing the delta with the $max(Power)$. The resulting $delta_{norm}$ should now be comparable between installations of different sizes. Choosing to reject every day for which $delta_{norm}$ value is higher than 0.05 would eliminate days with higher than 5\% normalized noise.


%By choosing to reject every day for which the $delta_{norm}$ value is higher than 0.05, we can eliminate days where the measured power values and low-pass filtered power values deviate on average more than 5\% from one another,

%from max-normalized power.



%the low pass filtered measured power values. With the Helsinki and Kuopio datasets threshold values as low as 0.5\% still provided some outputs. This threshold value returns more days for the Helsinki dataset, indicating a comparably lower base level of noice in the Helsinki data. This shows that threshold values are installation specific and determined by installation specific noise patterns.


%The thresholds between datasets are not directly comparable. Installation specific shading patterns may increase the base level of noise present in datasets and thus higher thresholds 



%While the The algorithm has some weak points. 



%As the method is based on calculating the delta from a smooth approximation, percentual delta values are unlikely to approach zero unless the modeled data itself is faulty. Depending on the data quality, higher threshold values may need to be used. For example, if the panels are shaded by a tree or some other obstruction during a short period each day, then base level of noise in the measurements would be higher than it is for the FMI datasets. 


%This could be circumvented by 


%As the algorithm is designed for finding the very smoothest of days, the algorithm generates large amounts of false negatives. When this is combined with the interference from sources other than clouds, 


%, the high number of false negatives 


%And as it is designed to seek the very smoothest of days, resulting in a large amount of false negatives. In addition, other forms of interefence such as snow on panels, reflective surfaces near solar panels and clouds which do not block direct sunlight may distract the algorithm. From a human perspective the results of the algorithm may thus be unintuitive as a snowy day may be classified as cloudy even though the interference source was snow on the panels and not the cloud cover. But from the perspective of the processing algorithms, the source of noise in the data is irrelevant.

%Despite these differences


%The terms used in signal processing have mathematical counterparts with some distinctions, for example we can use mathematical structures such as lists, matrices or graphs as signals. And the mathematical near equivalent of high and low frequencies could be defined with the help of delta values between neighboring numerical values in these structures. If the delta values between any two near by values is significant, then this section of the mathematical structure contains high frequency change. Similarly if delta values between any two distant values are high, this is indicative of low frequency change between these two points.






%\noindent There are two mathematically non-trivial components in the algorithm. The first is low pass filtering, a process which treats cloudy and cloud free days differently. This is a concept borrowed from the field of signal processing and the differential treatment can be used to aid in classification. The second non-trivial part is measuring the change between filtered and unfiltered measurements. If the change is minimal, the day can be classified as cloud free.

%The term low pass filtering can be used for any process which takes an input and eliminates frequencies which are higher than a chosen cut off frequency, allowing lower frequencies to pass. With the power measurements, a simple low pass filter could be a running average filter which calculates a new power value as the average of the last $n$ values. The low pass filtering used in this thesis and shown in \ref{fig_cloudfree_algo} was accomplished with discrete Fourier transformations. Discrete Fourier transformations change a list of numbers into a set of sine and cosine waves of different amplitudes, the sum of which forms a continuous representation of the discrete values. This continuous representation can be sampled in order to return the data back into discrete values and by selectively choosing only the waves with longest wavelengths, discrete Fourier transformation can be used for low pass filtering. Good results were achieved by using the 5 to 7 of the longest frequencies generated by the fast Fourier transformation algorithm. 

% n-th degree polynomial approximation of the measurements or new and filtered measurement values could be defined as the average of nearest 20 power measurements.


% The second non-trivial component is measuring the delta between filtered and unfiltered days. This is accomplished by calculating the lenghts of point to point curves in Euclidean space represented by the measurements and filtered measurements by using the equation $\Sigma_{i=1}^{n}  \sqrt[2]{1+ (powers[i]-powers[i-1])^2}$. If the curve lenghts differ by more than x percent, the day can be classified as cloudy.

%The second non-trivial component is measuring the delta between filtered and unfiltered days. This is accomplished by first computing the per minute delta $\delta[i] = |p[i] - p_{lp}[i]|$ where $p[i]$ is the measured power value at index $i$ and $p_{lp}[i]$ is the corresponding power measurement after low pass filter was applied. These delta values can then be used for calculating the average delta per measurement $\delta_{avg} = (\delta[1]+\delta[2] + \dots+ \delta[n])/n$. This normalization is important as without normalization days with lower temporal resolution or shorter day lengths would have lower delta values. The delta value $\delta_{avg}$ can be normalized further as $\delta_n=\delta_{avg}/p_{max}$. This final normalization step results in a delta which represents per measurement deviation as a fraction of max and this final value is comparable between installations of different sizes.

%Final part of cloud free day detection is choosing a threshold value. By manually testing different threshold values, the lowest values which still returns days with the Helsinki and Kuopio datasets are 0.3 and 0.5 respectively. This difference is small but it shows that there is a measurable difference between the datasets and that threshold value should be chosen based on the datasets used. Another issue with the algorithm rises from temporal resolution, if temporal resolution is 1 measurement per 15 minutes and if the reported value is an average of last measurement period, a low pass filter has already been applied. This would not make the algorithm unsuitable for its purpose, but threshold values would have to be adjusted. One method for generalizing the algorithm and avoiding the forementioned issues would be sorting the days based on their smoothness values and then choosing the $n$ smoothest days to operate on. Such modifications may be necessary or beneficial when operating with large amounts of different datasets, but as of now they have not been implemented.


%Most of the program code required for cloud free day detection is included in appendix \ref{cloudfree_code} and the complete source is available on github \cite{github_source}.

%s based on their apparent smoothness values and then operating with the 

%The second non-trivial component is measuring the delta between filtered and unfiltered days. This is accomplished by calculating the lenghts of point to point curves in euclidean space represented by the measurements and filtered measurements by using the equation $\Sigma_{i=1}^{n}  \sqrt[2]{1+ (powers[i]-powers[i-1])^2}$. If the curve lenghts differ by more than x percent


%length of a day by calculating euclidean distance over a day of measurements with the sum function: $\Sigma_{i=1}^{n}  \sqrt[2]{1+ (powers[i]-powers[i-1])^2}$



%Here the algorithm first computes the per measurement error $\delta_i$ for each measurement $p_i$ in the list of powers so that $\delta_i = |p_i - {p_i}_{lp}|$ . This results in a new list of error values is then used in orde to compute the average deviation from 



%Then the average deviation is calculated $\delta_{avg} = (\delta_1+\delta_2 + \dots+ \delta_n)/n$. Finally the delta can be normalized to 



%Here the chosen method is based on computing the length of the discrete curve in minute-power space and the equation used is the sum of point to point distances in Euclidean space. The 



%Alternative approaches such as measuring the total travel on power-axis and calculating error area between filtered and unfiltered curves could also be used. 










%The algorithm splits data into individual days of measurements and the days are evaluated separately. This means that 

%Another important form of data pre-processing is the selection of days which are suitable for model fitting. These so called cloud free or clear days can be distinguished by their smooth power measurement curves as seen in \ref{fig_cloudfree_vs_cloudy}, but the strength of cloud induced noise can be stronger or weaker as well. By making the assumption that clouds induce randomness into the power measurements, we can attempt to measure this randomness and use it as a metric for deciding whether a day is suitable for model fitting or not.

%\noindent To borrow terminology and tools from the field of signal processing, a clear difference between cloudy and cloud free days is the presense of high-frequency noise. Noise means that the alteration to the signal is unwanted and high-frequency signifies that the alteration results in a signal where measurements differ from neighboring measurements. In the case of solar PV power measurements, power $p_t$ for minute $t$ is more likely to differ from the average of the nearest 10 or 100 measurements if the day is cloudy than when the day is cloud free. This difference is visible in \ref{fig_cloudfree_algo} where a cloud free and cloudy day have both been locally averaged.

%Locally averaging or low pass filtering the values can be done in multiple ways. For example, the filtered values could be defined as the average of the nearest 50 values or a polynomial of sufficient degree could be fitted to the measurements data. Regardless of the method used, the important aspect is that the averaging function has to eliminate the apparent randomness from the signal while cloud free days should be hard to distinguish from their unfiltered counterparts. 

%Here the chosen low pass filtering method uses discrete Fourier transform. Fourier transformations are a method of representing number series and functions as sine and cosine waves. In short, the sum of a sine and a cosine wave of frequency $f$ can be used to construct a new sine wave of frequency $f$ with chosen phase. Fourier series combine this property and multiple frequencies in order to construct a frequency representation of the approximated data. As Fourier transformations start from the computation of the longest frequencies, the first $n$ outputs can be used to approximate the general shape of the data.

%This method of filtering may seem complicated as the same results could be achieved by defining new power values as $p_{t_{new}} = avg(p_{t-n}, ...,p_{t-1},  p_t,p_{t+1}, ..., p_{t+n})$ but this would require the turning of the window size parameter $n$ which is dependent on temporal resolution. The benefit of the Fourier series based method is that it is independent from temporal resolution and unlike Taylors polynomial or other localized approximation methods, discrete Fourier series approach does not favor a certain data interval.


\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{pics/cloudfree_algo}
\figcaption{Cloud free day finder low pass filtering phase.}
\label{fig_cloudfree_algo}
\end{figure}



\newpage






%\noindent \textbf{Note:} that the algorithm listed above is highly dependent on measurement intervals and further tuning could be needed when operating with datasets that have different temporal resolutions. And as is, the algorithm selects days based on their proportionally low high frequency component, thus in theory this algorithm should classify zero power output days as cloud free days. Despite this fault the algorithm seems to work well for the FMI datasets.% as long as the input are selected to contain days close or between the spring and fall equinoxes. 



%The implementation of the clear sky algorithm seems to work well as indicated by \ref{fig-multidaypoavsmeasurements} but there are a few weak points in the algorithm as well. For example if a constant power day is given as the input for the algorithm, the algorithm will classify it as clear sky day even if a constant power day is more likely to be the result of faulty measuring instruments or errors in data preprocessing than a real cloud free day. In addition, the algorithm is unlikely to work well if sections are either removed from the measurements or if there is significant shading affecting the power output of the installation.

%\subsection{Difference between solar days and UTC days}
%For solar power analysis the concept of solar days is fairly useful. Solar days and sun based time measurement systems tend to rely on the angle of the sun and three 


%The timestamps used in solar PV measurements can be assumed to be in UTC +0 time. While this means that timezones or daylight saving time do not have to be accounted for, some operations may become more complicated as well since UTC days and solar days at do not always align. Note that here local solar day is defined as the 

%For example, if a one day slice is taken from a $0^\circ$ longitude installation power generation data, it is rather likely that the solar power generation would occur during an interval which centers around noon or 720 minutes. If the same slice is taken at $90^\circ$ longitude, this generation would be shifted by approximately 360 minutes. This is perfectly normal and expected behavior, but as a result, determining the first and last non-zero minutes of the day can be seen to nontrivial as per figure \ref{fig_poa0vs90}. In the $0^\circ$ plot, the first and last non-zero minutes are approximately 330 and 1100, but should the first and last minutes of the $90^\circ$ plot be defined as 0 and 750, 0 and 1420, -19 and 750 or something else entirely?


%\begin{figure}[h!]
%\centering
%\includegraphics[width=0.9\linewidth]{pics/poa0vs90}
%\figcaption{Approximations of solar power generation at $0^\circ$ and $90^\circ$ longitude.}
%\label{fig_poa0vs90}
%\end{figure}


%\section{Third party datasets}
%Sunny portal etc here

%\section{Required assumptions}The algorithms presented in this thesis work on datasets which 


%Due to the diversity of possible solar PV installations, creating an universal model for parameter estimation is not fesiable. Installations could have been modified during operation, system failures could induce different types of changes in measurement data and 




%\section{Assumptions and possible issues}
%If metadata such as the geographic location or panel installation angles is missing from the datafiles, it is very likely that other critical pieces of information could be left out as well. Were additional modules were installed during operation? Could some panels be installed at different angles? What if the panels are installed in tracking mounts and thus the panel angles vary during each day? These questions are left unaswered and thus some assumptions have to be made. In this thesis we will assume that the panels are installed on fixed mounts, no changes were done during data gathering period and all panels are oriented similarly. We will also assume that there are no major obstacles casting shadows on the panels and that the panels are not self-shadowing, meaning that the panels are not casting shadows on one another.

%Another source of uncertainty is data collection itself. The device responsible for measuring power output values and logging the values has to have a clock for measuring time, but this clock could have be running too slow or fast, resulting in a drifting error in the timestamps. Similarly if the system clock is running at the right speed but it is off by a minute or two, this could cause a bias in the data which would be hard to detect. There is also the question of how measurement timing is done. If the time resolution of the logging device is 15 minutes, is the power value at 12:45 taken during the 45th minute or is the power value the average of the previous 15 minutes as is often done in meteorology? Or could the power value be the average of measurements taken during the interval 12:38 to 12:52? In meoteorology, the last period average would be the standard, but standards may not always be followed.

%is the standard, but standards are not always followed and thus 

%If enough time and effort was spent on algorithm design, in theory it could be possible to detect modifications to PV systems, the presence of variable panel angle systems and clock drifts. But these topics are outside the scope of this thesis and thus the assumption will be made that the 





















